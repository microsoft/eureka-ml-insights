{
  "Geometric Reasoning (GeoMeter)": {
    "experiments": [
      {
        "title": "Depth and Height",
        "categories": [
          "depth",
          "height"
        ],
        "series": [
          {
            "title": "Depth and Height",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  42.6,
                  17.0
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  50.8,
                  28.0
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  47.7,
                  33.0
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  39.0,
                  15.4
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  39.4,
                  13.0
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  43.9,
                  18.0
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  37.1,
                  15.0
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Multimodel Question Answering (MMMU)": {
    "experiments": [
      {
        "title": "Categories",
        "categories": [
          "Art and Design",
          "Business",
          "Health and Medicine",
          "Humanities and Social Science",
          "Science",
          "Tech and Engineering"
        ],
        "series": [
          {
            "title": "Categories",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  53.3,
                  40.0,
                  45.3,
                  64.2,
                  36.7,
                  41.4
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  71.7,
                  62.0,
                  64.0,
                  71.7,
                  48.7,
                  47.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  60.8,
                  51.3,
                  53.3,
                  60.0,
                  46.0,
                  41.4
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  71.7,
                  52.0,
                  64.7,
                  74.2,
                  48.0,
                  44.3
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  64.2,
                  38.7,
                  46.0,
                  65.8,
                  35.3,
                  33.8
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  73.3,
                  46.7,
                  66.0,
                  76.7,
                  53.3,
                  44.3
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  55.0,
                  41.3,
                  47.3,
                  56.7,
                  26.7,
                  35.7
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Image Understanding": {
    "experiments": [
      {
        "title": "Object Recognition",
        "categories": [
          "correct"
        ],
        "series": [
          {
            "title": "Object Recognition (Single)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  72.3
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  86.7
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  88.1
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  78.3
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  74.8
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  86.5
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  81.5
                ]
              }
            ]
          },
          {
            "title": "Object Recognition (Pairs)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  46.4
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  67.8
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  78.0
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  57.0
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  53.0
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  70.2
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  68.0
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Object Detection",
        "categories": [
          "AP50"
        ],
        "series": [
          {
            "title": "Object Detection (Single)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  0.2
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  3.1
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  3.0
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  0.5
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  0.7
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  4.5
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  8.8
                ]
              }
            ]
          },
          {
            "title": "Object Detection (Pairs)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  0.5
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  3.9
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  6.6
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  1.2
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  0.7
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  11.9
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  10.6
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Visual Prompting",
        "categories": [
          "correct"
        ],
        "series": [
          {
            "title": "Visual Prompting (Single)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  76.1
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  84.3
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  86.3
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  68.1
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  63.4
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  76.9
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  84.5
                ]
              }
            ]
          },
          {
            "title": "Visual Prompting (Pairs)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  11.2
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  62.7
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  77.6
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  51.8
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  36.9
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  69.2
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  66.7
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Spatial Reasoning",
        "categories": [
          "correct"
        ],
        "series": [
          {
            "title": "Spatial Reasoning (Single)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  44.9
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  86.2
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  75.2
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  66.6
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  68.4
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  87.6
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  91.2
                ]
              }
            ]
          },
          {
            "title": "Spatial Reasoning (Pairs)",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  50.5
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  98.8
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  69.5
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  88.0
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  85.5
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  99.2
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  79.8
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Vision Language Understanding": {
    "experiments": [
      {
        "title": "Spatial-Map Understanding",
        "categories": [
          "image_only",
          "image_text",
          "text_only"
        ],
        "series": [
          {
            "title": "Spatial-Map Understanding",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  34.3,
                  59.5,
                  57.7
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  46.5,
                  62.5,
                  61.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  58.0,
                  64.3,
                  65.8
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  25.9,
                  55.4,
                  56.0
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  23.1,
                  48.9,
                  48.5
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  61.1,
                  64.3,
                  64.7
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  33.8,
                  46.1,
                  46.2
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Maze Navigation",
        "categories": [
          "image_only",
          "image_text",
          "text_only"
        ],
        "series": [
          {
            "title": "Maze Navigation",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  16.2,
                  33.5,
                  34.3
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  34.4,
                  36.9,
                  38.7
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  50.1,
                  38.5,
                  33.3
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  26.3,
                  30.5,
                  29.7
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  25.9,
                  32.3,
                  18.5
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  41.1,
                  41.0,
                  33.6
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  36.9,
                  37.3,
                  37.5
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Grid Counting",
        "categories": [
          "image_only",
          "image_text",
          "text_only"
        ],
        "series": [
          {
            "title": "Grid Counting",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  44.9,
                  89.7,
                  79.1
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  84.5,
                  95.5,
                  94.5
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  86.3,
                  86.2,
                  86.9
                ]
              },
              {
                "name": "GPT-4-Turbo-2024-04-09",
                "scores": [
                  61.4,
                  94.0,
                  94.0
                ]
              },
              {
                "name": "GPT-4-Vision-Preview",
                "scores": [
                  67.6,
                  92.9,
                  92.9
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  83.5,
                  98.6,
                  98.1
                ]
              },
              {
                "name": "Llava-1_6-34B",
                "scores": [
                  20.1,
                  27.0,
                  38.9
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Instruction Following (IFEval)": {
    "experiments": [
      {
        "title": "Instruction Following",
        "categories": [
          "change_case",
          "combination",
          "detectable_content",
          "detectable_format",
          "keywords",
          "language",
          "length_constraints",
          "punctuation",
          "startend"
        ],
        "series": [
          {
            "title": "Instruction Following",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  85.4,
                  93.8,
                  100.0,
                  89.8,
                  85.3,
                  100.0,
                  76.2,
                  90.9,
                  86.6
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  87.6,
                  89.2,
                  100.0,
                  84.1,
                  86.5,
                  93.5,
                  81.1,
                  89.4,
                  86.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  82.0,
                  90.8,
                  92.5,
                  91.1,
                  79.1,
                  67.7,
                  66.4,
                  92.4,
                  89.6
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  76.4,
                  80.0,
                  94.3,
                  91.7,
                  82.2,
                  96.8,
                  72.7,
                  57.6,
                  95.5
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  82.0,
                  87.7,
                  92.5,
                  90.4,
                  84.7,
                  100.0,
                  74.8,
                  93.9,
                  95.5
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  86.5,
                  80.0,
                  94.3,
                  88.5,
                  79.8,
                  71.0,
                  73.4,
                  95.5,
                  98.5
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  88.8,
                  92.3,
                  98.1,
                  91.1,
                  85.9,
                  96.8,
                  74.1,
                  97.0,
                  98.5
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  87.6,
                  87.7,
                  98.1,
                  90.4,
                  85.3,
                  93.5,
                  70.6,
                  97.0,
                  94.0
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  82.0,
                  83.1,
                  96.2,
                  92.4,
                  79.1,
                  96.8,
                  75.5,
                  84.8,
                  85.1
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Long Context (FlenQA)": {
    "experiments": [
      {
        "title": "Context Size",
        "categories": [
          "250",
          "500",
          "1000",
          "2000",
          "3000"
        ],
        "series": [
          {
            "title": "Context Size",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  95.6,
                  88.7,
                  78.5,
                  74.1,
                  73.0
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  95.4,
                  91.6,
                  82.4,
                  77.2,
                  75.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  97.8,
                  96.5,
                  88.8,
                  79.7,
                  75.8
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  99.2,
                  96.4,
                  93.5,
                  88.9,
                  85.5
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  99.3,
                  97.2,
                  96.2,
                  94.1,
                  90.8
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  99.0,
                  95.1,
                  87.5,
                  77.7,
                  74.9
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  99.7,
                  98.2,
                  97.3,
                  95.2,
                  92.5
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  99.3,
                  97.4,
                  94.9,
                  91.6,
                  86.4
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  97.4,
                  94.5,
                  88.2,
                  77.4,
                  66.7
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Paragraph Location",
        "categories": [
          "first",
          "last",
          "middle",
          "random"
        ],
        "series": [
          {
            "title": "Paragraph Location",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  90.4,
                  84.6,
                  80.8,
                  72.8
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  91.6,
                  89.5,
                  81.4,
                  75.4
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  84.6,
                  93.9,
                  87.6,
                  84.8
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  95.1,
                  96.2,
                  91.5,
                  88.0
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  98.4,
                  97.5,
                  96.3,
                  94.1
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  97.4,
                  96.2,
                  92.7,
                  89.3
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  90.5,
                  85.3,
                  81.6,
                  81.9
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Information Retrieval (Kitab)": {
    "experiments": [
      {
        "title": "No Context",
        "categories": [
          "city-name",
          "ends-with",
          "human-name",
          "publishing-year",
          "starts-with",
          "word-count"
        ],
        "series": [
          {
            "title": "Satisfaction Rate",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  54.8,
                  23.8,
                  57.0,
                  43.6,
                  46.2,
                  64.8
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  57.4,
                  33.9,
                  57.7,
                  48.6,
                  50.4,
                  71.5
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  42.0,
                  18.2,
                  42.3,
                  44.2,
                  37.6,
                  42.2
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  52.6,
                  14.2,
                  50.4,
                  43.0,
                  39.3,
                  60.1
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  53.8,
                  30.6,
                  57.8,
                  45.2,
                  55.8,
                  71.6
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  44.1,
                  28.3,
                  41.4,
                  31.8,
                  31.7,
                  43.5
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  55.6,
                  40.1,
                  59.8,
                  48.4,
                  54.2,
                  68.1
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  46.2,
                  32.8,
                  49.8,
                  39.3,
                  41.7,
                  51.4
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  41.8,
                  10.3,
                  43.0,
                  33.0,
                  37.1,
                  39.7
                ]
              }
            ]
          },
          {
            "title": "Completeness",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  17.1,
                  11.5,
                  17.9,
                  25.1,
                  31.9,
                  7.5
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  18.7,
                  7.3,
                  19.1,
                  30.5,
                  30.2,
                  7.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  12.2,
                  5.0,
                  12.7,
                  13.5,
                  11.8,
                  0.9
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  22.2,
                  11.5,
                  24.9,
                  29.9,
                  33.6,
                  12.1
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  21.0,
                  10.4,
                  24.6,
                  26.5,
                  25.8,
                  8.0
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  13.9,
                  5.1,
                  14.3,
                  20.6,
                  28.2,
                  5.3
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  12.3,
                  5.7,
                  14.0,
                  25.8,
                  29.5,
                  5.6
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  12.9,
                  6.8,
                  14.3,
                  23.5,
                  27.5,
                  5.7
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  17.6,
                  6.8,
                  17.8,
                  23.7,
                  26.5,
                  7.5
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "Self Context",
        "categories": [
          "city-name",
          "ends-with",
          "human-name",
          "publishing-year",
          "starts-with",
          "word-count"
        ],
        "series": [
          {
            "title": "Satisfaction Rate",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  46.5,
                  32.7,
                  58.4,
                  42.2,
                  59.0,
                  54.5
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  53.4,
                  38.6,
                  63.7,
                  51.5,
                  64.0,
                  61.4
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  37.6,
                  14.9,
                  48.0,
                  33.8,
                  44.9,
                  42.2
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  42.5,
                  13.0,
                  50.8,
                  40.9,
                  49.7,
                  49.6
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  46.0,
                  39.7,
                  58.1,
                  43.2,
                  58.9,
                  55.7
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  34.7,
                  16.9,
                  43.3,
                  33.9,
                  37.6,
                  45.6
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  55.1,
                  43.8,
                  65.2,
                  50.2,
                  57.7,
                  68.2
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  44.3,
                  28.9,
                  58.2,
                  42.6,
                  52.2,
                  55.0
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  35.5,
                  12.5,
                  43.6,
                  30.6,
                  45.1,
                  38.5
                ]
              }
            ]
          },
          {
            "title": "Completeness",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  23.0,
                  22.1,
                  23.5,
                  28.4,
                  37.2,
                  12.0
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  22.4,
                  20.3,
                  25.1,
                  31.5,
                  36.1,
                  12.4
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  20.8,
                  11.6,
                  20.4,
                  20.1,
                  31.4,
                  6.6
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  24.9,
                  13.9,
                  27.5,
                  32.7,
                  33.5,
                  14.0
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  23.8,
                  22.8,
                  27.1,
                  30.7,
                  33.3,
                  11.0
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  17.7,
                  4.8,
                  18.4,
                  20.3,
                  33.0,
                  7.0
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  20.3,
                  11.7,
                  23.8,
                  28.2,
                  37.8,
                  5.6
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  16.8,
                  7.5,
                  19.3,
                  19.9,
                  26.6,
                  6.1
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  19.1,
                  7.6,
                  19.9,
                  25.8,
                  28.0,
                  9.6
                ]
              }
            ]
          }
        ]
      },
      {
        "title": "With Context",
        "categories": [
          "city-name",
          "ends-with",
          "human-name",
          "publishing-year",
          "starts-with",
          "word-count"
        ],
        "series": [
          {
            "title": "Satisfaction Rate",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  75.1,
                  50.7,
                  85.8,
                  96.8,
                  90.0,
                  76.6
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  74.7,
                  58.6,
                  88.0,
                  97.8,
                  90.5,
                  84.0
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  65.8,
                  34.7,
                  82.7,
                  90.2,
                  88.4,
                  53.9
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  66.3,
                  20.3,
                  81.8,
                  86.2,
                  75.3,
                  73.7
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  78.2,
                  52.2,
                  85.7,
                  93.7,
                  89.3,
                  80.0
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  75.9,
                  23.1,
                  82.1,
                  87.1,
                  87.0,
                  69.7
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  79.6,
                  55.4,
                  90.9,
                  96.3,
                  90.2,
                  90.0
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  76.2,
                  42.3,
                  87.1,
                  96.4,
                  90.9,
                  81.5
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  74.3,
                  19.6,
                  83.3,
                  92.6,
                  84.6,
                  62.6
                ]
              }
            ]
          },
          {
            "title": "Completeness",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  59.7,
                  53.5,
                  65.1,
                  86.7,
                  81.8,
                  31.3
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  60.4,
                  46.7,
                  72.5,
                  94.8,
                  77.0,
                  33.5
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  61.2,
                  33.7,
                  55.5,
                  91.6,
                  86.2,
                  16.0
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  62.2,
                  30.7,
                  72.7,
                  89.4,
                  84.8,
                  43.8
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  61.3,
                  54.5,
                  74.0,
                  90.8,
                  79.3,
                  36.7
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  56.6,
                  20.4,
                  68.9,
                  88.2,
                  83.1,
                  24.2
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  57.9,
                  40.8,
                  73.7,
                  96.5,
                  85.9,
                  24.8
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  58.4,
                  23.2,
                  72.3,
                  84.5,
                  82.5,
                  22.7
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  62.9,
                  25.9,
                  75.0,
                  84.5,
                  84.1,
                  30.0
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "Toxicity Detection (Toxigen)": {
    "experiments": [
      {
        "title": "Discriminative (9k)",
        "categories": [
          "asian",
          "black",
          "chinese",
          "jewish",
          "latino",
          "lgbtq",
          "mental_dis",
          "mexican",
          "middle_east",
          "muslim",
          "native_american",
          "physical_dis",
          "women"
        ],
        "series": [
          {
            "title": "Toxicity Detection",
            "values": [
              {
                "name": "Claude-3-Opus",
                "scores": [
                  52.0,
                  39.1,
                  56.5,
                  41.1,
                  42.2,
                  47.6,
                  64.0,
                  53.5,
                  57.2,
                  51.0,
                  56.0,
                  64.5,
                  64.7
                ]
              },
              {
                "name": "Claude-3_5-Sonnet",
                "scores": [
                  63.7,
                  50.2,
                  72.4,
                  42.5,
                  61.6,
                  67.9,
                  76.1,
                  72.5,
                  73.3,
                  66.7,
                  67.8,
                  79.4,
                  82.6
                ]
              },
              {
                "name": "Gemini-1_5-Pro",
                "scores": [
                  38.5,
                  28.5,
                  51.0,
                  23.1,
                  32.9,
                  38.5,
                  54.5,
                  46.3,
                  48.1,
                  25.4,
                  40.2,
                  62.0,
                  62.5
                ]
              },
              {
                "name": "GPT-4-1106-Preview",
                "scores": [
                  82.3,
                  81.2,
                  82.6,
                  73.7,
                  80.0,
                  85.2,
                  84.5,
                  86.5,
                  85.9,
                  84.7,
                  85.0,
                  86.9,
                  93.3
                ]
              },
              {
                "name": "GPT-4o-2024-05-13",
                "scores": [
                  83.5,
                  83.5,
                  84.8,
                  76.2,
                  82.3,
                  88.0,
                  88.0,
                  88.3,
                  89.1,
                  83.6,
                  88.9,
                  88.2,
                  94.1
                ]
              },
              {
                "name": "Llama-3-70B",
                "scores": [
                  85.5,
                  85.6,
                  85.3,
                  80.8,
                  84.5,
                  87.7,
                  87.0,
                  87.9,
                  89.8,
                  88.8,
                  89.3,
                  90.1,
                  93.9
                ]
              },
              {
                "name": "Llama-3_1-405B",
                "scores": [
                  54.0,
                  41.4,
                  59.6,
                  39.2,
                  60.8,
                  48.2,
                  69.6,
                  61.5,
                  62.6,
                  52.9,
                  60.0,
                  63.8,
                  69.2
                ]
              },
              {
                "name": "Llama-3_1-70B",
                "scores": [
                  85.2,
                  84.9,
                  83.9,
                  78.9,
                  82.5,
                  84.5,
                  83.5,
                  88.2,
                  88.8,
                  87.6,
                  89.0,
                  87.9,
                  92.6
                ]
              },
              {
                "name": "Mistral_Large_2_2407",
                "scores": [
                  81.5,
                  79.7,
                  81.2,
                  77.3,
                  80.7,
                  86.3,
                  85.4,
                  88.6,
                  88.1,
                  80.4,
                  82.8,
                  87.4,
                  92.5
                ]
              }
            ]
          }
        ]
      }
    ]
  }
}