{
  "Geometric Reasoning": {
    "graphs": [
      {
        "title": "Depth",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 42.4
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 50.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 47.5
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 38.9
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 39.4
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 43.9
          },
          {
            "name": "Llava-1_6-34B",
            "score": 36.9
          }
        ]
      },
      {
        "title": "Height",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 17.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 28.0
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 33.0
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 12.0
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 13.0
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 18.0
          },
          {
            "name": "Llava-1_6-34B",
            "score": 15.0
          }
        ]
      }
    ]
  },
  "MMMU": {
    "graphs": [
      {
        "title": "Art and Design",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 53.3
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 71.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 60.8
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 71.7
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 64.2
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 73.3
          },
          {
            "name": "Llava-1_6-34B",
            "score": 55.0
          }
        ]
      },
      {
        "title": "Business",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 40.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 62.0
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 51.3
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 52.0
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 38.7
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 46.7
          },
          {
            "name": "Llava-1_6-34B",
            "score": 41.3
          }
        ]
      },
      {
        "title": "Health and Medicine",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 45.3
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 64.0
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 53.3
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 64.7
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 46.0
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 66.0
          },
          {
            "name": "Llava-1_6-34B",
            "score": 47.3
          }
        ]
      },
      {
        "title": "Humanities and Social Science",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 64.2
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 71.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 60.0
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 74.2
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 65.8
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 76.7
          },
          {
            "name": "Llava-1_6-34B",
            "score": 56.7
          }
        ]
      },
      {
        "title": "Science",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 36.7
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 48.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 46.0
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 48.0
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 35.3
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 53.3
          },
          {
            "name": "Llava-1_6-34B",
            "score": 26.7
          }
        ]
      },
      {
        "title": "Tech and Engineering",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 41.4
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 47.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 41.4
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 44.3
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 33.8
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 44.3
          },
          {
            "name": "Llava-1_6-34B",
            "score": 35.7
          }
        ]
      }
    ]
  },
  "Image Understanding": {
    "graphs": []
  },
  "Vision Language Understanding": {
    "graphs": []
  },
  "IFEval": {
    "graphs": []
  },
  "FlenQA": {
    "graphs": [
      {
        "title": "Context Size: 250",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 95.6
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 95.4
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 97.8
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 99.2
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 99.3
          },
          {
            "name": "Llama-3-70B",
            "score": 99.0
          },
          {
            "name": "Llama-3_1-405B",
            "score": 99.7
          },
          {
            "name": "Llama-3_1-70B",
            "score": 99.3
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 97.4
          }
        ]
      },
      {
        "title": "Context Size: 500",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 88.7
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 91.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 96.5
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 96.4
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 97.2
          },
          {
            "name": "Llama-3-70B",
            "score": 95.1
          },
          {
            "name": "Llama-3_1-405B",
            "score": 98.2
          },
          {
            "name": "Llama-3_1-70B",
            "score": 97.4
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 94.5
          }
        ]
      },
      {
        "title": "Context Size: 1000",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 78.5
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 82.4
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 88.8
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 93.5
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 96.2
          },
          {
            "name": "Llama-3-70B",
            "score": 87.5
          },
          {
            "name": "Llama-3_1-405B",
            "score": 97.3
          },
          {
            "name": "Llama-3_1-70B",
            "score": 94.9
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 88.2
          }
        ]
      },
      {
        "title": "Context Size: 2000",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 74.1
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 77.2
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 79.7
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 88.9
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 94.1
          },
          {
            "name": "Llama-3-70B",
            "score": 77.7
          },
          {
            "name": "Llama-3_1-405B",
            "score": 95.2
          },
          {
            "name": "Llama-3_1-70B",
            "score": 91.6
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 77.4
          }
        ]
      },
      {
        "title": "Context Size: 3000",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 73.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 75.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 75.8
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 85.5
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 90.8
          },
          {
            "name": "Llama-3-70B",
            "score": 74.9
          },
          {
            "name": "Llama-3_1-405B",
            "score": 92.5
          },
          {
            "name": "Llama-3_1-70B",
            "score": 86.4
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 66.7
          }
        ]
      }
    ]
  },
  "Kitab": {
    "graphs": []
  },
  "Toxigen": {
    "graphs": []
  }
}