{
  "language": {
    "capabilities": [
      {
        "name": "Information Retrieval Fact Recall",
        "description": "Task: Retrieving long-form information from the model's parametric knowledge or from given input context with filtering constraints. Capability importance: All information retrieval tasks involve some form of constraint that defines the retrieval query. However, other simpler IR benchmarks only test for short-form generation (finding a single fact) and for a single con-straint. Being able to answer more complex queries is relevant to advanced search and information finding. State-of-the-art: Constrained retrieval from parametric knowledge is still prone to major irrelevance and fact fabrication with constraint satisfaction being less than 60%. Constrained retrieval from given input context is significantly better in overall, but for queries with more than one constraint constraint satisfaction and completeness drop to less than 70% and 60% respectively.",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 18.7
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 20.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 9.8
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 23.3
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 20.3
          },
          {
            "name": "Llama-3-70B",
            "score": 15.0
          },
          {
            "name": "Llama-3_1-405B",
            "score": 16.8
          },
          {
            "name": "Llama-3_1-70B",
            "score": 16.0
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 17.6
          }
        ]
      },
      {
        "name": "Information Retrieval Fact Precision",
        "description": "Task: Retrieving long-form information from the model's parametric knowledge or from given input context with filtering constraints. Capability importance: All information retrieval tasks involve some form of constraint that defines the retrieval query. However, other simpler IR benchmarks only test for short-form generation (finding a single fact) and for a single con-straint. Being able to answer more complex queries is relevant to advanced search and information finding. State-of-the-art: Constrained retrieval from parametric knowledge is still prone to major irrelevance and fact fabrication with constraint satisfaction being less than 60%. Constrained retrieval from given input context is significantly better in overall, but for queries with more than one constraint constraint satisfaction and completeness drop to less than 70% and 60% respectively.",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 50.5
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 55.3
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 41.3
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 47.0
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 53.7
          },
          {
            "name": "Llama-3-70B",
            "score": 37.4
          },
          {
            "name": "Llama-3_1-405B",
            "score": 54.9
          },
          {
            "name": "Llama-3_1-70B",
            "score": 44.2
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 36.3
          }
        ]
      },
      {
        "name": "Instruction Following",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 81.9
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 81.3
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 75.2
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 75.2
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 81.3
          },
          {
            "name": "Llama-3-70B",
            "score": 77.3
          },
          {
            "name": "Llama-3_1-405B",
            "score": 83.5
          },
          {
            "name": "Llama-3_1-70B",
            "score": 80.8
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 77.3
          }
        ]
      },
      {
        "name": "Long Context QA Average",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 82.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 84.5
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 87.7
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 92.7
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 95.5
          },
          {
            "name": "Llama-3-70B",
            "score": 86.8
          },
          {
            "name": "Llama-3_1-405B",
            "score": 96.6
          },
          {
            "name": "Llama-3_1-70B",
            "score": 93.9
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 84.8
          }
        ]
      },
      {
        "name": "Long Context QA Longest Context (3K)",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 73.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 75.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 75.8
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 85.5
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 90.8
          },
          {
            "name": "Llama-3-70B",
            "score": 74.9
          },
          {
            "name": "Llama-3_1-405B",
            "score": 92.5
          },
          {
            "name": "Llama-3_1-70B",
            "score": 86.4
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 66.7
          }
        ]
      },
      {
        "name": "Toxicity Detection",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 53.2
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 67.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 42.6
          },
          {
            "name": "GPT-4-1106-Preview",
            "score": 84.1
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 86.1
          },
          {
            "name": "Llama-3-70B",
            "score": 87.4
          },
          {
            "name": "Llama-3_1-405B",
            "score": 57.1
          },
          {
            "name": "Llama-3_1-70B",
            "score": 86.0
          },
          {
            "name": "Mistral_Large_2_2407",
            "score": 84.1
          }
        ]
      }
    ]
  },
  "multimodal": {
    "capabilities": [
      {
        "name": "Geometric Reasoning",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 40.1
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 48.6
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 46.1
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 36.5
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 36.9
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 41.5
          },
          {
            "name": "Llava-1_6-34B",
            "score": 34.9
          }
        ]
      },
      {
        "name": "Multimodal QA",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 45.7
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 59.3
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 50.9
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 57.2
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 45.2
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 58.0
          },
          {
            "name": "Llava-1_6-34B",
            "score": 42.4
          }
        ]
      },
      {
        "name": "Object Recognition",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 72.3
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 86.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 88.1
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 78.3
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 74.8
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 86.5
          },
          {
            "name": "Llava-1_6-34B",
            "score": 81.5
          }
        ]
      },
      {
        "name": "Object Detection (AP50)",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 0.2
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 3.1
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 3.0
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 0.5
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 0.7
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 4.5
          },
          {
            "name": "Llava-1_6-34B",
            "score": 8.8
          }
        ]
      },
      {
        "name": "Visual Prompting",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 76.1
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 84.3
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 86.3
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 68.1
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 63.4
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 76.9
          },
          {
            "name": "Llava-1_6-34B",
            "score": 84.5
          }
        ]
      },
      {
        "name": "Spatial Reasoning",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 44.9
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 86.2
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 75.2
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 66.6
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 68.4
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 87.6
          },
          {
            "name": "Llava-1_6-34B",
            "score": 91.2
          }
        ]
      },
      {
        "name": "Spatial Map Understanding",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 50.5
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 56.9
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 62.7
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 45.8
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 40.2
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 63.4
          },
          {
            "name": "Llava-1_6-34B",
            "score": 42.0
          }
        ]
      },
      {
        "name": "Navigation",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 28.0
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 36.7
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 40.6
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 28.8
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 25.6
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 38.6
          },
          {
            "name": "Llava-1_6-34B",
            "score": 37.2
          }
        ]
      },
      {
        "name": "Counting in a Grid",
        "description": "TBD",
        "models": [
          {
            "name": "Claude-3-Opus",
            "score": 71.2
          },
          {
            "name": "Claude-3_5-Sonnet",
            "score": 91.5
          },
          {
            "name": "Gemini-1_5-Pro",
            "score": 86.5
          },
          {
            "name": "GPT-4-Turbo-2024-04-09",
            "score": 83.1
          },
          {
            "name": "GPT-4-Vision-Preview",
            "score": 84.4
          },
          {
            "name": "GPT-4o-2024-05-13",
            "score": 93.4
          },
          {
            "name": "Llava-1_6-34B",
            "score": 28.7
          }
        ]
      }
    ]
  }
}