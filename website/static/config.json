{
  "benchmarks": [
    {
      "name": "Geometric Reasoning",
      "modality": "multimodal",
      "filePattern": ".*GeoMCQMetric_result_grouped_by_category_report.*",
      "description": "Cool stuff",
      
      "graphs": [
        {
          "title": "Depth",
          "path": ["Geometric-Reasoning"],
          "metric": ["GeoMCQMetric_result", "depth"]
        },
        {
          "title": "Height",
          "path": ["Geometric-Reasoning"],
          "metric": ["GeoMCQMetric_result", "height"]
        }
      ]
    },
    {
      "name": "MMMU",
      "modality": "multimodal",
      "path": ["MMMU", "MMMU"],
      "filePattern": ".*MMMUMetric_result_grouped_by_category_normalized_report.*",
      "description": "",
      "graphs": [
        {
          "title": "Art and Design",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Art and Design", "correct"]
        },
        {
          "title": "Business",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Business", "correct"]
        },
        {
          "title": "Health and Medicine",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Health and Medicine", "correct"]
        },
        {
          "title": "Humanities and Social Science",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Humanities and Social Science", "correct"]
        },
        {
          "title": "Science",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Science", "correct"]
        },
        {
          "title": "Tech and Engineering",
          "path": ["MMMU", "MMMU"],
          "metric": ["MMMUMetric_result", "Tech and Engineering", "correct"]
        }
      ]
    },
    {
      "name": "Image Understanding",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "filePattern": ".*GeoMCQMetric_result_grouped_by_category_report.*",
      "description": "",
      "graphs": [

      ]
    },
    {
      "name": "Vision Language Understanding",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "description": "",
      "filePattern": ".*",
      "graphs": [
        
      ]
    },
    {
      "name": "IFEval",
      "modality": "language",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "description": "",
      "filePattern": ".*",
      "graphs": [
        
      ]
    },
    {
      "name": "FlenQA",
      "modality": "language",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "description": "",
      "filePattern": ".*ExactMatch_result_grouped_by_ctx_size.*",
      "graphs": [
        {
          "title": "Context Size: 250",
          "path": ["FlenQA"],
          "metric": ["ExactMatch_result", "250", "correct"]
        },
        {
          "title": "Context Size: 500",
          "path": ["FlenQA"],
          "metric": ["ExactMatch_result", "500", "correct"]
        }
        ,
        {
          "title": "Context Size: 1000",
          "path": ["FlenQA"],
          "metric": ["ExactMatch_result", "1000", "correct"]
        }
        ,
        {
          "title": "Context Size: 2000",
          "path": ["FlenQA"],
          "metric": ["ExactMatch_result", "2000", "correct"]
        }
        ,
        {
          "title": "Context Size: 3000",
          "path": ["FlenQA"],
          "metric": ["ExactMatch_result", "3000", "correct"]
        }
      ]
    },
    {
      "name": "Kitab",
      "modality": "language",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "description": "",
      "filePattern": ".*",
      "graphs": [
        
      ]
    },
    {
      "name": "Toxigen",
      "modality": "language",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "description": "",
      "filePattern": ".*",
      "graphs": [
        
      ]
    }
  ],
  "capability_mapping": [
    {
      "capability": "Information Retrieval Fact Recall",
      "modality": "language",
      "path": ["kitab", "ONE_BOOK_CONSTRAINT_PIPELINE"],
      "metric": ["KitabMetric_satisfied_rate"],
      "run": "average",
      "description": "Task: Retrieving long-form information from the model's parametric knowledge or from given input context with filtering constraints. Capability importance: All information retrieval tasks involve some form of constraint that defines the retrieval query. However, other simpler IR benchmarks only test for short-form generation (finding a single fact) and for a single con-straint. Being able to answer more complex queries is relevant to advanced search and information finding. State-of-the-art: Constrained retrieval from parametric knowledge is still prone to major irrelevance and fact fabrication with constraint satisfaction being less than 60%. Constrained retrieval from given input context is significantly better in overall, but for queries with more than one constraint constraint satisfaction and completeness drop to less than 70% and 60% respectively."
    },
    {
      "capability": "Information Retrieval Fact Precision",
      "modality": "language",
      "path": ["kitab", "ONE_BOOK_CONSTRAINT_PIPELINE"],
      "metric": ["KitabMetric_completeness"],
      "run": "average",
      "description": "Task: Retrieving long-form information from the model's parametric knowledge or from given input context with filtering constraints. Capability importance: All information retrieval tasks involve some form of constraint that defines the retrieval query. However, other simpler IR benchmarks only test for short-form generation (finding a single fact) and for a single con-straint. Being able to answer more complex queries is relevant to advanced search and information finding. State-of-the-art: Constrained retrieval from parametric knowledge is still prone to major irrelevance and fact fabrication with constraint satisfaction being less than 60%. Constrained retrieval from given input context is significantly better in overall, but for queries with more than one constraint constraint satisfaction and completeness drop to less than 70% and 60% respectively."
    },
    {
      "capability": "Instruction Following",
      "modality": "language",
      "path": ["IFEval"],
      "metric": ["IFEvalMetric_strict_follow_all_instructions"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Long Context QA Average",
      "modality": "language",
      "path": ["FlenQA"],
      "metric": ["ExactMatch_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Long Context QA Longest Context (3K)",
      "modality": "language",
      "path": ["FlenQA"],
      "metric": ["ExactMatch_result", "3000", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Toxicity Detection",
      "modality": "language",
      "path": ["Toxigen", "Discriminative", "9K"],
      "metric": ["ExactMatch_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Geometric Reasoning",
      "modality": "multimodal",
      "path": ["Geometric-Reasoning"],
      "metric": ["GeoMCQMetric_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Multimodal QA",
      "modality": "multimodal",
      "path": ["MMMU", "MMMU"],
      "metric": ["MMMUMetric_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Object Recognition",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_RECOGNITION_SINGLE"],
      "metric": ["ObjectRecognitionMetric_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Object Detection (AP50)",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "OBJECT_DETECTION_SINGLE"],
      "metric": ["CocoObjectDetectionMetric_result", "AP50"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Visual Prompting",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "VISUAL_PROMPTING_SINGLE"],
      "metric": ["ObjectRecognitionMetric_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Spatial Reasoning",
      "modality": "multimodal",
      "path": ["IMAGE_UNDERSTANDING", "SPATIAL_REASONING_SINGLE"],
      "metric": ["SpatialAndLayoutReasoningMetric_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Spatial Map Understanding",
      "modality": "multimodal",
      "path": ["VISION_LANGUAGE", "SPATIAL_MAP"],
      "metric": ["SpatialAndLayoutReasoningMetricMap_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Navigation",
      "modality": "multimodal",
      "path": ["VISION_LANGUAGE", "MAZE"],
      "metric": ["SpatialAndLayoutReasoningMetricMaze_result", "correct"],
      "run": "average",
      "description": "TBD"
    },
    {
      "capability": "Counting in a Grid",
      "modality": "multimodal",
      "path": ["VISION_LANGUAGE", "SPATIAL_GRID"],
      "metric": ["SpatialAndLayoutReasoningMetricGrid_result", "correct"],
      "run": "average",
      "description": "TBD"
    }
  ],
  "model_families": [
    "claude", "gemini", "llama", "gpt", "mistral", "llava"
  ],
  "model_list": [
    {
      "model_family": "Claude",
      "model": "Claude-3-Opus",
      "color": "#4762D6",
      "modality": ["vision", "language"]
    },
    {
      "model_family": "Claude",
      "model": "Claude-3_5-Sonnet",
      "color": "#4762D6",
      "modality": ["vision", "language"]
    },
    {
      "model_family": "Gemini",
      "model": "Gemini-1_5-Pro",
      "color": "#AE8C00",
      "modality": ["vision", "language"]
    },
    {
      "model_family": "Llama",
      "model": "Llama-3-70B",
      "color": "#058801",
      "modality": ["language"]
    },
    {
      "model_family": "Llama",
      "model": "Llama-3_1-405B",
      "color": "#71A920",
      "modality": ["language"]
    },
    {
      "model_family": "Llama",
      "model": "Llama-3_1-70B",
      "color": "#65D060",
      "modality": ["language"]
    },
    {
      "model_family": "GPT",
      "model": "GPT-4-1106-Preview",
      "color": "#E3008C",
      "modality": ["language"]
    },
    {
      "model_family": "GPT",
      "model": "GPT-4o-2024-05-13",
      "color": "#B61A40",
      "modality": ["vision", "language"]
    },
    {
      "model_family": "GPT",
      "model": "GPT-4-Vision-Preview",
      "color": "#E3008C",
      "modality": ["vision"]
    },
    {
      "model_family": "GPT",
      "model": "GPT-4-Turbo-2024-04-09",
      "color": "#B61A40",
      "modality": ["vision"]
    },
    {
      "model_family": "Mistral",
      "model": "Mistral_Large_2_2407",
      "color": "#E86B24",
      "modality": ["language"]
    },
    {
      "model_family": "Llava",
      "model": "Llava-1_6-34B",
      "color": "#E86B24",
      "modality": ["language"]
    }
  ]
}